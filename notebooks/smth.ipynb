{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twnews.utils.memoize import load, dump\n",
    "from twnews.utils.sparse_math import get_similarity_matrix\n",
    "from twnews.dataset.text_to_text_relation import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset = load(os.path.join('/home/avybornov/twnews_data_april/gold_dataset', 'dataset_auto_0.5'))\n",
    "# self = dataset\n",
    "                            \n",
    "# lemmatizer = Lemmatizer()\n",
    "# index = 0\n",
    "# for _news in self.news.get_documents():\n",
    "#     _news.index = index\n",
    "#     index += 1\n",
    "\n",
    "# for tweet in self.tweets.get_documents():\n",
    "#     tweet.words = lemmatizer.split_text_to_lemmas(tweet.text)\n",
    "#     tweet.index = index\n",
    "#     index += 1\n",
    "\n",
    "# #print len(tweets), len(news)\n",
    "\n",
    "# similarity_matrix = get_similarity_matrix(self.get_documents(), self.get_documents(), self.corpus, self.tf_idf_matrix)\n",
    "# news = self.news.get_documents()\n",
    "# tweets = self.tweets.get_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(self.get_documents()), len(self.corpus), self.tf_idf_matrix.shape\n",
    "# print similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k = 10\n",
    "# tweet_to_tweet_hashtags = get_tweet_to_tweet_hashtags_relation(tweets, k, similarity_matrix)\n",
    "# print 'num of tweet to tweet by hashtags relation', len(tweet_to_tweet_hashtags)\n",
    "\n",
    "# NE_set = get_NE_from_news(news)\n",
    "# tweet_to_tweet_NER = get_tweet_to_tweet_NER_relation(tweets, NE_set, k, similarity_matrix)\n",
    "# print 'num of tweet to tweet by NER relation', len(tweet_to_tweet_NER)\n",
    "\n",
    "# tweet_to_tweet_time = get_document_to_documet_time_relation(tweets, k, similarity_matrix)\n",
    "# print 'num of tweet to tweet by time relation', len(tweet_to_tweet_time)\n",
    "\n",
    "# news_to_news_time = get_document_to_documet_time_relation(news, k, similarity_matrix)\n",
    "# print 'num of news to news by time relation', len(news_to_news_time)\n",
    "\n",
    "# total_relations = filter_links(tweet_to_tweet_hashtags | tweet_to_tweet_NER | tweet_to_tweet_time | news_to_news_time)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_auto_0.5\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 19\n",
      "num of tweet to tweet by NER relation 87\n",
      "num of tweet to tweet by time relation 81\n",
      "num of news to news by time relation 16283\n",
      "num of total relations 16410\n",
      "-------\n",
      "dataset_cutted_0.5\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 182\n",
      "num of tweet to tweet by NER relation 635\n",
      "num of tweet to tweet by time relation 477\n",
      "num of news to news by time relation 4470\n",
      "num of total relations 5402\n",
      "-------\n",
      "dataset_manual_0.5\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 66\n",
      "num of tweet to tweet by NER relation 297\n",
      "num of tweet to tweet by time relation 244\n",
      "num of news to news by time relation 16216\n",
      "num of total relations 16644\n",
      "-------\n",
      "dataset_total_0.5\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 182\n",
      "num of tweet to tweet by NER relation 675\n",
      "num of tweet to tweet by time relation 514\n",
      "num of news to news by time relation 16208\n",
      "num of total relations 17193\n",
      "-------\n",
      "dataset_auto_0.0\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 844\n",
      "num of tweet to tweet by NER relation 3921\n",
      "num of tweet to tweet by time relation 2943\n",
      "num of news to news by time relation 16086\n",
      "num of total relations 21894\n",
      "-------\n",
      "dataset_cutted_0.0\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 1507\n",
      "num of tweet to tweet by NER relation 6260\n",
      "num of tweet to tweet by time relation 5336\n",
      "num of news to news by time relation 4370\n",
      "num of total relations 14069\n",
      "-------\n",
      "dataset_manual_0.0\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 170\n",
      "num of tweet to tweet by NER relation 762\n",
      "num of tweet to tweet by time relation 692\n",
      "num of news to news by time relation 16175\n",
      "num of total relations 17305\n",
      "-------\n",
      "dataset_total_0.0\n",
      "preparation finished\n",
      "num of tweet to tweet by hashtags relation 1507\n",
      "num of tweet to tweet by NER relation 6821\n",
      "num of tweet to tweet by time relation 5461\n",
      "num of news to news by time relation 15988\n",
      "num of total relations 26136\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "dirname = \"/home/avybornov/twnews_data_april/gold_dataset\"\n",
    "datasets = ['dataset_auto_0.5', 'dataset_cutted_0.5', 'dataset_manual_0.5', 'dataset_total_0.5',\n",
    "'dataset_auto_0.0', 'dataset_cutted_0.0', 'dataset_manual_0.0', 'dataset_total_0.0'] #'dataset_auto_0.0'\n",
    "#datasets = ['dataset_auto_0.0']\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print dataset_name\n",
    "    dataset = load(os.path.join(dirname, dataset_name))\n",
    "    \n",
    "    dataset.init_text_to_text_links()\n",
    "    \n",
    "    dump(dataset, os.path.join(\"/home/avybornov/tmp/datasets\", dataset_name))\n",
    "    print '-------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump(dataset, 'test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
