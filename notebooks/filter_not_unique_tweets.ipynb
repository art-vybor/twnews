{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from twnews.utils.memoize import load\n",
    "from twnews.utils.text_processors import Lemmatizer\n",
    "\n",
    "\n",
    "dataset = load('dataset')\n",
    "tweets_storage = dataset.tweets\n",
    "news_storage = dataset.news\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "print tweets_storage.length()\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweets_unique(tweet, news_storage, percent_of_unique_words):\n",
    "    tweet_words = lemmatizer.split_text_to_lemmas_without_lemmatize(tweet.text)\n",
    "    \n",
    "    for url in tweet.urls:\n",
    "        if news_storage.exists(url):\n",
    "            single_news = dataset.news.get(url)\n",
    "            single_news_words = lemmatizer.split_text_to_lemmas_without_lemmatize(single_news.title)\n",
    "            \n",
    "            unique_words = len([word for word in tweet_words if word not in single_news_words])\n",
    "            unique_words_normalized = unique_words*1.0/len(tweet_words)\n",
    "            if unique_words_normalized >= percent_of_unique_words:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_not_unique_tweets(self, news_storage, percent_of_unique_words):\n",
    "    for tweet_id, tweet in self.dataset_texts_dict.items():\n",
    "        if not tweets_unique(tweet, news_storage, percent_of_unique_words):\n",
    "            del self.dataset_texts_dict[tweet_id]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 | 0.0\n",
      "1449/1600 | 0.1\n",
      "1338/1600 | 0.2\n",
      "1206/1600 | 0.3\n",
      "1088/1600 | 0.4\n",
      "976/1600 | 0.5\n",
      "766/1600 | 0.6\n",
      "560/1600 | 0.7\n",
      "343/1600 | 0.8\n",
      "127/1600 | 0.9\n",
      "69/1600 | 1.0\n"
     ]
    }
   ],
   "source": [
    "for percent_of_unique_words in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]:\n",
    "    tweets_storage_copy = deepcopy(tweets_storage)\n",
    "    filter_not_unique_tweets(tweets_storage_copy, news_storage, percent_of_unique_words)\n",
    "    print '%s/%s | %s' % (tweets_storage_copy.length(), tweets_storage.length(), percent_of_unique_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
