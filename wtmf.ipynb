{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-23 00:43:11.076892: Start of loading news from /home/avybornov/twnews_data/rss\n",
      "2016-03-23 00:43:11.268799: News successfully loaded\n",
      "Политолог: Россия стремится к миру в Сирии, сокращая группировку войск\n"
     ]
    }
   ],
   "source": [
    "from twnews.dataset.storage import NewsStorage\n",
    "news_storage = NewsStorage()\n",
    "news_texts = news_storage.get_texts()\n",
    "print news_texts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tvf.fit_transform(news_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tfidf_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = tvf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29200\n",
      "[u'\\u043a\\u043e\\u0432\\u0442\\u0443\\u043d', u'\\u043a\\u043e\\u0433\\u0430\\u043b\\u044b\\u043c\\u0430\\u0432\\u0438\\u0430', u'\\u043a\\u043e\\u0433\\u0430\\u043d', u'\\u043a\\u043e\\u0433\\u0434\\u0430', u'\\u043a\\u043e\\u0433\\u043e', u'\\u043a\\u043e\\u0434', u'\\u043a\\u043e\\u0434\\u0430', u'\\u043a\\u043e\\u0434\\u0435\\u043a\\u0441', u'\\u043a\\u043e\\u0434\\u0435\\u043a\\u0441\\u0430', u'\\u043a\\u043e\\u0434\\u0435\\u043a\\u0441\\u0435']\n"
     ]
    }
   ],
   "source": [
    "print len(corpus)\n",
    "s = 10000\n",
    "print corpus[s:s+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Политолог: Россия стремится к миру в Сирии, сокращая группировку войск\n",
      "-------------------\n",
      "Политолог\n",
      ":\n",
      "Россия\n",
      "стремится\n",
      "к\n",
      "миру\n",
      "в\n",
      "Сирии\n",
      ",\n",
      "сокращая\n",
      "группировку\n",
      "войск\n",
      "-------------------\n",
      "Политолог\n",
      ":\n",
      "Россия\n",
      "стремится\n",
      "миру\n",
      "Сирии\n",
      ",\n",
      "сокращая\n",
      "группировку\n",
      "войск\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#https://pythonprogramming.net/stop-words-nltk-tutorial/?completed=/tokenizing-words-sentences-nltk-tutorial/\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "text = news_texts[0]\n",
    "word_tokens = [wordnet_lemmatizer.lemmatize(w.decode('utf-8')) for w in word_tokenize(text)]\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print text\n",
    "#print word_tokens\n",
    "#print (map(unicode, word_tokens))\n",
    "print '-------------------'\n",
    "for x in word_tokens: print x\n",
    "print '-------------------'\n",
    "for x in filtered_sentence: print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
